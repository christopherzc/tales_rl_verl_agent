#!/usr/bin/env bash
set -euo pipefail

########################################
# Basic argument handling
########################################

ARG1=${1:-0}
ARG2=${2:-0}
ARG3=${3:-""}

env_seed=$ARG1
agent_port_dir="$HOME/server_logs/remote_gym${ARG2}/remote_gym"

echo "Starting local single-node job..."
echo "Arguments: ARG1=$ARG1, ARG2=$ARG2, ARG3=$ARG3"

########################################
# Model / reward / prompt config
########################################

# model_name=ccui46/qwen3-8b-warmstart
model_name=Qwen/Qwen3-8B
# model_name=ccui46/qwen3-8b-warmstart-single-prompt-traj-traces-100
# model_name=ccui46/qwen3-8b-warmstart-single-prompt
# model_name=ccui46/qwen3-8b-warmstart-single-prompt-trained-traj
# model_name=ccui46/qwen3-8b-warmstart-single-prompt-fixed-gt

# reward_mode=go_new_prompt_no_actionf_lower_lr
reward_mode=go_new_prompt_no_actionf_contrib_penalty
reward_mode=go

# reward_mode=native_new_prompt_lower_lr_contrib_cs_token
# reward_mode=native_new_prompt_lower_lr_contrib
# reward_mode=native_new_prompt
# reward_mode=native_ccp_pogs
# reward_mode=native_ccp_pogs
# reward_mode=native_ccp_csp_np

prompt_template=safe_textworld_1_6_baseline__goalrewonly_penalty_on_gen_start_stop
prompt_template=safe_textworld_1_6_baseline_penalty_on_gen_start_stop
# prompt_template=safe_textworld_1_6_baseline_gor_penalty_on_gen_start_stop_with_think
# prompt_template=safe_textworld_1_6_baseline_penalty_on_gen_start_stop_with_think
prompt_template=safe_textworld_1_6_baseline_pogs_ccp
prompt_template=safe_textworld_1_6_baseline_pogs_ccp_with_think
prompt_template=safe_textworld_1_6_go_pogs_ccp
prompt_template=safe_textworld_1_6_pogs_ccp_with_think
# prompt_template=safe_tw_1_6_go_pogs_ccp
prompt_template=basecase
prompt_template=admissible
# prompt_template=safe_tw_go_ccp_pogs_with_think

export PYTHONUNBUFFERED=1

echo "Using agent port dir: $agent_port_dir, prompt template: $prompt_template, reward mode: $reward_mode"

########################################
# Env vars (kept from original)
########################################

export HYDRA_FULL_ERROR=1
export VLLM_ATTENTION_BACKEND=FLASH_ATTENTION_2   # not XFORMERS
# export VLLM_ATTENTION_BACKEND=XFORMERS
export VLLM_USE_V1=0
export NCCL_ASYNC_ERROR_HANDLING=1
export NCCL_NET_GDR_LEVEL=2
export NCCL_P2P_DISABLE=0

export NCCL_SOCKET_NTHREADS=8
export NCCL_NSOCKS_PERTHREAD=2
export NCCL_CROSS_NIC=1
export TORCH_NCCL_HIGH_PRIORITY=1

export NCCL_IB_DISABLE=0
export NCCL_IB_GDR_LEVEL=2
export NCCL_MIN_NCHANNELS=4
export NCCL_NET_GDR_LEVEL=2
export CUDA_DEVICE_MAX_CONNECTIONS=1

export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8

verl_workdir=$HOME/

DATA_DIR="$HOME/data/verl-agent/text"
ENGINE=vllm
TRAIN_PARQUET="$DATA_DIR/train.parquet"
VAL_PARQUET="$DATA_DIR/test.parquet"

# To fix a triton cache issue, setting a node-local cache directory
export TRITON_CACHE_DIR="/tmp/triton-cache/$(hostname)"
mkdir -p "$TRITON_CACHE_DIR"

########################################
# Training hyperparameters
########################################

offload=False

n_resp_per_prompt_val=1
total_epochs=200
save_freq=-1
test_freq=10
max_ckpt_to_keep=1
enable_curriculum=True
val_before_train=True
train_prompt_bsz=8
val_prompt_bsz=16
max_prompt_length=$((1024 * 15))
max_response_length=512
max_total_length=$((max_prompt_length + max_response_length))
num_nodes=1            # single machine
micro_bs_per_gpu=8
micro_batch_per_gpu=32

pwd

echo "Preprocessing dataâ€¦"
uv run --no-project -m examples.data_preprocess.prepare \
    --mode text \
    --train_data_size "${train_prompt_bsz}" \
    --val_data_size "${val_prompt_bsz}"

########################################
# PPO training
########################################

uv run --no-project -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \
    data.train_files="${TRAIN_PARQUET}" \
    data.val_files="${VAL_PARQUET}" \
    data.train_batch_size="${train_prompt_bsz}" \
    data.val_batch_size="${val_prompt_bsz}" \
    data.max_prompt_length="${max_prompt_length}" \
    data.max_response_length="${max_response_length}" \
    data.filter_overlong_prompts=True \
    data.truncation='left' \
    data.return_raw_chat=True \
    actor_rollout_ref.actor.fsdp_config.reshard_after_forward=False \
    actor_rollout_ref.ref.fsdp_config.reshard_after_forward=False \
    actor_rollout_ref.actor.fsdp_config.param_offload=True \
    actor_rollout_ref.actor.fsdp_config.optimizer_offload=True \
    actor_rollout_ref.model.path="${model_name}" \
    \
    actor_rollout_ref.model.enable_activation_offload=True \
    actor_rollout_ref.model.enable_gradient_checkpointing=True \
    actor_rollout_ref.model.use_remove_padding=True \
    actor_rollout_ref.model.use_fused_kernels=True \
    actor_rollout_ref.ref.ulysses_sequence_parallel_size=1 \
    actor_rollout_ref.actor.ulysses_sequence_parallel_size=1 \
    \
    actor_rollout_ref.actor.use_torch_compile=False \
    \
    actor_rollout_ref.actor.ppo_max_token_len_per_gpu="${max_total_length}" \
    actor_rollout_ref.actor.strategy="fsdp" \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu="${micro_bs_per_gpu}" \
    actor_rollout_ref.actor.use_kl_loss=False \
    actor_rollout_ref.actor.use_dynamic_bsz=True \
    \
    +actor_rollout_ref.actor.use_entropy_advantage=False \
    +actor_rollout_ref.actor.entropy_advantage_alpha=0.01 \
    \
    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu="${micro_bs_per_gpu}" \
    +actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu="${micro_bs_per_gpu}" \
    actor_rollout_ref.rollout.log_prob_max_token_len_per_gpu="${max_total_length}" \
    \
    +actor_rollout_ref.actor.fsdp_config.sharding_strategy="FULL_SHARD" \
    +actor_rollout_ref.actor.fsdp_config.backward_prefetch="BACKWARD_PRE" \
    +critic.model.fsdp_config.sharding_strategy="FULL_SHARD" \
    +critic.model.fsdp_config.backward_prefetch="BACKWARD_PRE" \
    \
    actor_rollout_ref.ref.log_prob_max_token_len_per_gpu="${max_total_length}" \
    actor_rollout_ref.rollout.tensor_model_parallel_size=1 \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.3 \
    actor_rollout_ref.rollout.dtype='auto' \
    \
    actor_rollout_ref.rollout.max_num_batched_tokens=20000 \
    actor_rollout_ref.rollout.temperature=0.8 \
    actor_rollout_ref.ref.log_prob_use_dynamic_bsz=True \
    actor_rollout_ref.rollout.mode="sync" \
    actor_rollout_ref.rollout.enforce_eager=False \
    actor_rollout_ref.rollout.free_cache_engine=False \
    actor_rollout_ref.actor.fsdp_config.fsdp_size=8 \
    critic.optim.lr=1e-5 \
    critic.model.path=Qwen/Qwen3-8B \
    critic.ppo_micro_batch_size_per_gpu="${micro_bs_per_gpu}" \
    critic.model.enable_activation_offload=True \
    critic.model.enable_gradient_checkpointing=True \
    critic.model.fsdp_config.reshard_after_forward=False \
    critic.model.use_remove_padding=True \
    critic.use_dynamic_bsz=True \
    \
    algorithm.use_kl_in_reward=False \
    env.env_name=tales_alfworld \
    env.seed="${env_seed}" \
    env.max_steps=50 \
    +env.prompt_template="${prompt_template}" \
    +env.reward_mode="${reward_mode}" \
    +env.num_envs_per_batch=1 \
    +env.agent_port_dir="${agent_port_dir}" \
    trainer.logger='["console","wandb"]' \
    trainer.log_val_generations=30 \
    trainer.project_name="tales_rl" \
    trainer.experiment_name="ppo_qwen3-8b_${prompt_template}" \
    trainer.val_before_train=True \
    trainer.n_gpus_per_node=8 \
    trainer.nnodes="${num_nodes}" \
    trainer.save_freq="${save_freq}" \
    trainer.test_freq="${test_freq}" \
    trainer.total_epochs="${total_epochs}" \
    +trainer.remove_previous_ckpt_in_save=True \
    trainer.max_actor_ckpt_to_keep=0 \
    trainer.max_critic_ckpt_to_keep=0 \
    +trainer.save_full_model=True \
    +trainer.consolidate_checkpoints=True \
    +trainer.save_only_on_master=True

